\subsection{ensemble learning}

\subsubsection{Bootstrap aggregating (bagging)}
\cite{breiman1996bagging} Bootstrap aggregating, often abbreviated as bagging, involves having each model in the ensemble vote with equal weight.

\subsubsection{Random forest}

Random forest \cite{breiman2001random}: operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes output by individual trees. 

\subsubsection{AdaBoost}



AdaBoost \cite{freund1997decision}, short for "Adaptive Boosting", in the sense that subsequent classifiers built are tweaked in favor of those instances misclassified by previous classifiers. 


\subsubsection{Bayesian model averaging}

\cite{hoeting1999bayesian} Bayesian model averaging (BMA) is a pratical ensemble technique that seeks to approximate the Bayes Optimal Classifier by sampling hypotheses from the hypothesis space, and combining them using Bayes' law.