An ensemble combines the prediction power of a set of individually trained classifiers $H$ to produce a single classifier. Ensembles often are more accuracy as compared to base classifiers in the ensemble. %For combining the outputs of base classifiers, there are two main types, i.e., weighting methods and meta-learning methods \cite{rokach2010ensemble}. %Popular ensemble methods include Bagging \cite{breiman1996bagging} and AdaBoost \cite{freund1997decision}.

We focus on investigating \emph{weighting} methods, as one of the two main methods discussed in \cite{rokach2010ensemble}, for combining the outputs of base classifiers. Let $M$ be the set of examples, $L$ be the set of labels, $K$ be the set of classifiers, $A_{M*}$ be the true labels, and $A_{MK}$ be the labels assigned by $K$ on $M$. We consider a generalized weighting method. Based on the labels $A_{MK}^\text{train}$ on training examples, weights $W$ and beliefs $B$ are learned, where each $w_k \in W$ is the weight for $k\in K$, and each $b_{l_il_j,k} \in B$ is the belief probability of true label $l_i$, given the label $l_j$ assigned by $k$. For each testing example $X$, the ensemble classifier $\tilde{h}(X)$ assigns votes on labels $L$ by the values $WB$ for testing labels, and return the label or UNKNOWN using majority voting with a threshold value $\bar{V} \in [0, 1]$.

The weight scheme is represented by a tuple $<F\text{-strategy}(\tilde{K}), C\text{-strategy}, B\text{-strategy}, \bar{V}>$.

The weight vector $W$ can be obtained using a \emph{two-step scheme}. First, $F\text{-strategy}$ returns a binary array $F$ about the chosen classifiers. Here a subset of top $\tilde{K}$ classifiers are picked with the highest diversity, which has shown to have positive relationship with the ensemble accuracy \cite{kuncheva2003measures}. We consider nine measures of diversity \cite{kuncheva2003measures}, i.e., \emph{disagreement}, \emph{correlation coefficient}, \emph{Q-statistic}, \emph{double-fault}, \emph{coincident failure diversity}, \emph{entropy}, \emph{interrater agreement}, \emph{Kohavi-Wolpert}, and \emph{generalized diversity}. Second, the weight coefficients $C$ are assigned by $C\text{-strategy}$. The \emph{equal assignment} just gives the same weight as in the canonical majority voting, where the \emph{performance-based assignment} \cite{opitz1996generating} assigns weights proportional to the accuracy performance of each classifier on $M$. The final weights is $W=F\cdot C$.

One can also obtain the weight coefficients $C$ by searching in the configuration space. We consider DEPSO, a cooperative group optimizer \cite{xie2014cooperative}, to obtain the optimal weights for $K$, and thus to know the lowest training error that can be achieved using weights on given $A_{MK}^\text{train}$. Without loss of generality, the weighting space is defined as $c_k\in [0, 1]$ for each weight coefficient on $k$. 

$B\text{-strategy}$ obtains the belief matrix $B$, by only considering forms that are independent on $k$. A \emph{basic form} of $B$ is a diagonal $L\times L$ sub-matrix for each $k$, which is equivalent to the one using in the canonical majority voting. A \emph{new form}, which is not included in \cite{rokach2010ensemble}, obtains the beliefs as the frequency of $(l_i, l_j)$ pairs, where $l_i=a_{mk} \in A_{Mk}^\text{train}$ and $l_j=a_{m*} \in A_{M*}^\text{train}$, on training examples.

In real-world usages of songbird identification, people would prefer a very higher precision rate, among the classified results on a big amount of testing examples. We accommodate the real-world requirement at the ensemble level, by defining one label as ``UNKNOWN'', to include those instances that are not surely classified. This is also has an implication for scalability, as the labels might change over time, and studies can be mainly performed on ``UNKNOWN'' instances.


%The basic idea is to assign a different weight (function) $w_i$ for each base classifier $h_i\in H$, based on training and true labels on training examples. For each testing instance $X$, the ensemble classifier predicts $\tilde{h}(X)$ outputs the class receiving the highest weighted votes, based on the labels obtained by the base classifiers.

%\textbf{Bagging} Bagging is an abbreviated term of bootstrap aggregating, which trains each member classifier on a random redistribution of the training set, and using these to get
%an aggregated predictor. It has shown that Bagging is effective on unstable learning algorithms that suffer from significant changes from small perturbations in the learning set.

%Bagging is an averaging method, and the principle is to build several models independently and then to average their predictions.

%\textbf{AdaBoost} AdaBoost is a short name for ``Adaptive Boosting''. AdaBoost can be seen as a broad extension of on-line prediction model in a general decision-theoretic setting. By adapting the multiplicative weight-update rule \cite{littlestone1994weighted}, In theory, boosting can be used to significantly reduce the error of ``weak'' classifiers. AdaBoost is adaptive and can turn weak classifiers into a strong one.

%We will try to exploit the differences between Bagging and Boosting methods: (1) Bagging only uses resampling whereas Boosting uses reweighting; (2) Bagging always uses the uniform distribution whereas Boosting might modify the distribution over examples or mislabels.

%In real-world usages of songbird identification, people would prefer nearly a 100\% precision rate, but can tolerate a  recall rate that is not very high, as the prediction accuracy cannot approach 100\%. Furthermore, scalability is strongly encouraged as the labels (bird species) might change over time. We will accommodate the real-world requirements at the ensemble level, by defining one label as ``UNKNOWN'', and try to improve the precision on all other known labels.

